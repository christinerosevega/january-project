{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Wellesley News for Article Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Christine Pourheydarian\n",
    "* January 20, 2021\n",
    "* virtual tools used: this Jupyter notebook, Selenium, Python 3, Anaconda Navigator, Webdriver. \n",
    "* Credits note: Much of this code is taken from a notebook shared by Francisca Moya Jimenez. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping will be done with Selenium. Selenium is mainly used for testing, but is also useful for scraping data from websites with dynamic HTML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Selenium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/rose/opt/anaconda3/lib/python3.8/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /Users/rose/opt/anaconda3/lib/python3.8/site-packages (from selenium) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The webdriver you use will depend on the browser you prefer to use. I downloaded Chrome's webdriver and \n",
    "stored it in a folder called \"driver\" that is in the same folder as this Jupyter notebook, so that it would be easy to find and so that the PATH would be simple. (More information on Chrome's WebDriver can be found here: https://chromedriver.chromium.org/ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "#you will need to customize the path based on what you name the webdriver you are using \n",
    "#and based on where you store it on your computer. \n",
    "driver = webdriver.Chrome(executable_path='driver/chromedriver') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that the Wellesley News urls took a very, very long time to load. One useful strategy for more efficient, \n",
    "timely scraping is to stop downloading resources when loading the website is taking a long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"eager\" page loading strategy means that only html content is downloaded and parsed. Used here, since the html content is all we need for getting the article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.page_load_strategy = 'eager' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'driver/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the website's pages take a long time to load, we will also use the time module's sleep function to pause our python program when we want to give the website a bit of time to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sure that the driver works and that you understand what it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver is what you can use to scrape data from a website. Pretend the driver is a little character in a video game that you will control. You will tell the driver what to do, in order to gather all the data you need to gather. You will tell the driver what to do by using code as instructions.\n",
    "\n",
    "The following line of code should open a new browser that goes to the site: https://thewellesleynews.com. \n",
    "If you are using ChromeDriver, it should say, \"Chrome is being controlled by automated test software\", which \n",
    "means that Chrome is being controlled by Selenium Webdriver in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://thewellesleynews.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_file_detector',\n",
       " '_is_remote',\n",
       " '_mobile',\n",
       " '_switch_to',\n",
       " '_unwrap_value',\n",
       " '_web_element_cls',\n",
       " '_wrap_value',\n",
       " 'add_cookie',\n",
       " 'application_cache',\n",
       " 'back',\n",
       " 'capabilities',\n",
       " 'close',\n",
       " 'command_executor',\n",
       " 'create_options',\n",
       " 'create_web_element',\n",
       " 'current_url',\n",
       " 'current_window_handle',\n",
       " 'delete_all_cookies',\n",
       " 'delete_cookie',\n",
       " 'desired_capabilities',\n",
       " 'error_handler',\n",
       " 'execute',\n",
       " 'execute_async_script',\n",
       " 'execute_cdp_cmd',\n",
       " 'execute_script',\n",
       " 'file_detector',\n",
       " 'file_detector_context',\n",
       " 'find_element',\n",
       " 'find_element_by_class_name',\n",
       " 'find_element_by_css_selector',\n",
       " 'find_element_by_id',\n",
       " 'find_element_by_link_text',\n",
       " 'find_element_by_name',\n",
       " 'find_element_by_partial_link_text',\n",
       " 'find_element_by_tag_name',\n",
       " 'find_element_by_xpath',\n",
       " 'find_elements',\n",
       " 'find_elements_by_class_name',\n",
       " 'find_elements_by_css_selector',\n",
       " 'find_elements_by_id',\n",
       " 'find_elements_by_link_text',\n",
       " 'find_elements_by_name',\n",
       " 'find_elements_by_partial_link_text',\n",
       " 'find_elements_by_tag_name',\n",
       " 'find_elements_by_xpath',\n",
       " 'forward',\n",
       " 'fullscreen_window',\n",
       " 'get',\n",
       " 'get_cookie',\n",
       " 'get_cookies',\n",
       " 'get_log',\n",
       " 'get_network_conditions',\n",
       " 'get_screenshot_as_base64',\n",
       " 'get_screenshot_as_file',\n",
       " 'get_screenshot_as_png',\n",
       " 'get_window_position',\n",
       " 'get_window_rect',\n",
       " 'get_window_size',\n",
       " 'implicitly_wait',\n",
       " 'launch_app',\n",
       " 'log_types',\n",
       " 'maximize_window',\n",
       " 'minimize_window',\n",
       " 'mobile',\n",
       " 'name',\n",
       " 'orientation',\n",
       " 'page_source',\n",
       " 'quit',\n",
       " 'refresh',\n",
       " 'save_screenshot',\n",
       " 'service',\n",
       " 'session_id',\n",
       " 'set_network_conditions',\n",
       " 'set_page_load_timeout',\n",
       " 'set_script_timeout',\n",
       " 'set_window_position',\n",
       " 'set_window_rect',\n",
       " 'set_window_size',\n",
       " 'start_client',\n",
       " 'start_session',\n",
       " 'stop_client',\n",
       " 'switch_to',\n",
       " 'switch_to_active_element',\n",
       " 'switch_to_alert',\n",
       " 'switch_to_default_content',\n",
       " 'switch_to_frame',\n",
       " 'switch_to_window',\n",
       " 'title',\n",
       " 'w3c',\n",
       " 'window_handles']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://thewellesleynews.com/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Wellesley News'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out how to scrape the Wellesley News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, figure out what data you want to collect, and study the html to figure out how that data you want to access is stored, and how to navigate to it. I used Chrome's Inspect tool.\n",
    "\n",
    "\n",
    "I wanted to collect scrape all the articles from 2013 to 2020. For each article, I wanted to collect its title, contents/body, category, author, and publication date\n",
    "Initial observations on the html of the articles on The Wellesley News website:\n",
    "* The articles are organized by year and pages within the year.\n",
    "*  The articles are contained in \\<article> tags.\n",
    "* Article titles have class \"entry-title\"\n",
    "* Article categories have class \"entry-category\". There can be multiple categories.\n",
    "* Article authors have class \"author\"\n",
    "* Article's paragraphs are in paragraphs in a div that has class \"entry-content\".\n",
    "* Article's publication date stored in a \\<time> tag that has class entry-date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create game plan: \n",
    "Scrape all the articles from 2013 to 2020. For each article, get its title, contents/body, category, author, and publication date. Store all article data in a json file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "\n",
    "def getNumPages():\n",
    "    \"\"\"Gets the total number of pages appearing on the bottom of the page\"\"\"\n",
    "    sleep(2)\n",
    "    return int(driver.find_elements_by_class_name('page-numbers')[3].text)\n",
    "\n",
    "def getTitle(article):\n",
    "    \"\"\"Returns an article's title as a string\"\"\"\n",
    "    return article.find_element_by_tag_name('h2').text\n",
    "\n",
    "def getAuthors(article):\n",
    "    \"\"\"Returns a list with an article's authors\"\"\"\n",
    "    allElem = []\n",
    "    for elem in article.find_elements_by_class_name('author'):\n",
    "        allElem.extend(elem.find_elements_by_tag_name('a'))\n",
    "    return [elem.text for elem in allElem if elem.text != '']\n",
    "\n",
    "def getCategories(article):\n",
    "    \"\"\"Returns a list with an article's categories\"\"\"\n",
    "    return article.find_element_by_class_name('entry-category').text.split(', ')\n",
    "\n",
    "def getLink(article):\n",
    "    \"\"\"Returns an article's link as a string\"\"\"\n",
    "    return article.find_element_by_class_name('read-more-link').get_attribute('href')\n",
    "\n",
    "def getText(article):\n",
    "    \"\"\"Returns an article's short text snippet as a string\"\"\"\n",
    "    return article.find_element_by_class_name('entry-summary').text[:-11]\n",
    "\n",
    "#Returns article's contents given article's url\n",
    "def getArticle(url):\n",
    "    driver.get(url)\n",
    "    sleep(2)\n",
    "    paragraphs = [p.text for p in driver\\\n",
    "                  .find_element_by_css_selector('.single-box.clearfix.entry-content')\\\n",
    "                  .find_elements_by_tag_name('p')]\n",
    "    return ' '.join(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticles(year):\n",
    "    \"\"\"Retrieves all articles in the Wellesley News website for a given year\"\"\"\n",
    "    allArticles = []\n",
    "    \n",
    "    # Navigate to page\n",
    "    driver.get('https://thewellesleynews.com/'+year+'/')\n",
    "    sleep(2)\n",
    "    pages = getNumPages()\n",
    "        \n",
    "    # Extract relevant article information\n",
    "    def getInfo(article):\n",
    "        \"\"\"Scrapes the title, authors, categories, url, date and text for each article\"\"\"\n",
    "        url = getLink(article)\n",
    "        return {'title':getTitle(article), 'authors':getAuthors(article), \\\n",
    "                'date':url[29:39], 'categories':getCategories(article), \\\n",
    "                'url':url,'text':getText(article)}\n",
    "    \n",
    "    # Visit and scrape pages\n",
    "    for page in range(1,pages+1):\n",
    "        if page == 1:\n",
    "            for article in driver.find_elements_by_tag_name('article'):\n",
    "                try:\n",
    "                    allArticles.append(getInfo(article))\n",
    "                except:\n",
    "                    # Close pop up window and run getInfo again\n",
    "                    driver.find_element_by_id('close-icon').click()\n",
    "                    allArticles.append(getInfo(article))\n",
    "        else:\n",
    "            driver.get('https://thewellesleynews.com/'+year+'/page/'+str(page)+'/')\n",
    "            sleep(2)\n",
    "            for article in driver.find_elements_by_tag_name('article'):\n",
    "                allArticles.append(getInfo(article))\n",
    "                       \n",
    "    return allArticles\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6cf2c1c83172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'2013'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#'2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020'#You can change the years based on what year you want to collect data from. I collected data from 2013 to 2020.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0marticles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetArticles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-19d2950bbecb>\u001b[0m in \u001b[0;36mgetArticles\u001b[0;34m(year)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://thewellesleynews.com/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNumPages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Extract relevant article information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3895a54b2494>\u001b[0m in \u001b[0;36mgetNumPages\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Gets the total number of pages appearing on the bottom of the page\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_elements_by_class_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'page-numbers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetTitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "for year in ['2013']: #'2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020'#You can change the years based on what year you want to collect data from. I collected data from 2013 to 2020. \n",
    "    articles.extend(getArticles(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that it got all of the correct article urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-4fbaec6eff45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleList = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all 2013 articles and making their contents into dictionaries and adding their content to a list that \n",
    "#will be dumped into a json file once we have done this for all years 2013 thru 2020\n",
    "for article in articles:\n",
    "    data=  {\n",
    "            \"title\": article['title'],\n",
    "            \"authors\": article['authors'], \n",
    "            \"date\": article['date'], \n",
    "            \"categories\": article['categories'], \n",
    "            \"url\": article['url'], \n",
    "            \"body\": getArticle(article['url']), \n",
    "            }\n",
    "    articleList.append(data)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that all data of articles from the given year were added.\n",
    "There are: \n",
    "171 articles from 2013\n",
    "555 articles from 2014\n",
    "467 articles from 2015 \n",
    "471 articles from 2016\n",
    "398 articles from 2017\n",
    "422 articles from 2018\n",
    "431 articles from 2019\n",
    "205 articles from 2020\n",
    "so the length of the article list should be\n",
    "171 + 555 + 467 + 471 + 398 + 422 + 431 + 205 = 3120, \n",
    "if data from all articles ranging between 2013 from 2020 was collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articleList) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articleList.json', 'w') as outfile:\n",
    "    json.dump(articleList, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitting the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
